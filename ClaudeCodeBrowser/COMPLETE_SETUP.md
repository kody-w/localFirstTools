# üöÄ Complete End-to-End Agent Browser Setup

## üì¶ What You Have

A complete, working agent browser system that **completely bypasses CORS** by:
1. **Fetch Agent** - Downloads HTML from any URL (server-side, no CORS)
2. **Converts to JSON** - Structured page data
3. **Proxy Server** - Serves JSON to web interface
4. **Web Interface** - Loads and works with JSON as virtual pages

## üéØ How It Works

```
URL ‚Üí Fetch Agent ‚Üí HTML ‚Üí Convert ‚Üí JSON ‚Üí Proxy Server ‚Üí Web Interface ‚Üí Display
                   (no CORS!)                    (serves)        (loads)
```

## üèÉ Quick Start (3 Steps)

###  Step 1: Install Dependencies
```bash
cd "AI Library/ClaudeCodeBrowser"
npm install
```

### Step 2: Start the Proxy Server
```bash
node proxy-server.js
```

Output:
```
üöÄ Agent Browser Proxy Server
üì° Running on http://localhost:3000
üåê Open http://localhost:3000 to use the interface
```

### Step 3: Open Web Interface
```bash
# In your browser, go to:
http://localhost:3000
```

**That's it! You're ready to browse without CORS restrictions!**

## üé® Usage

### In the Web Interface

1. **Enter a URL** in the navigation bar:
   ```
   https://example.com
   ```

2. **Click "Go"** - The system will:
   - Send request to proxy server
   - Proxy fetches HTML (bypasses CORS)
   - Converts to JSON
   - Returns structured data
   - Displays in interface

3. **Extract, Interact, Save** - Everything works normally!

## üõ†Ô∏è Advanced Usage

### Command Line - Fetch Single URL
```bash
# Fetch and display
node fetch-agent.js https://example.com

# Fetch and save to file
node fetch-agent.js https://example.com example.json
```

### Command Line - Batch Fetch
```bash
# Create urls.txt with one URL per line:
echo "https://example.com" > urls.txt
echo "https://wikipedia.org" >> urls.txt
echo "https://github.com" >> urls.txt

# Fetch all
node fetch-agent.js --batch urls.txt

# Results saved to ./pages/ directory
```

### API Endpoints

When proxy server is running:

#### Fetch URL (Bypasses CORS)
```bash
curl "http://localhost:3000/api/fetch?url=https://example.com"
```

#### List Cached Pages
```bash
curl "http://localhost:3000/api/cache"
```

#### List Saved Files
```bash
curl "http://localhost:3000/api/files"
```

#### Batch Fetch
```bash
curl -X POST http://localhost:3000/api/fetch-batch \
  -H "Content-Type: application/json" \
  -d '{"urls": ["https://example.com", "https://github.com"]}'
```

## üìä JSON Page Format

```json
{
  "url": "https://example.com",
  "fetchedAt": "2024-01-15T10:30:00.000Z",
  "statusCode": 200,
  "headers": {...},
  "content": {
    "title": "Example Domain",
    "meta": {
      "description": "...",
      "keywords": "..."
    },
    "headings": [
      {"level": 1, "text": "Example Domain", "html": "..."}
    ],
    "links": [
      {"href": "https://...", "text": "More info", "title": null}
    ],
    "images": [
      {"src": "https://...", "alt": "...", "title": null}
    ],
    "paragraphs": ["First paragraph...", "Second paragraph..."],
    "bodyText": "Full page text...",
    "forms": [
      {
        "action": "/submit",
        "method": "POST",
        "inputs": [...]
      }
    ],
    "rawHtml": "<html>...</html>",
    "hasScripts": true
  },
  "version": "1.0.0"
}
```

## üîß Configuration

### Change Port
```bash
PORT=8080 node proxy-server.js
```

### Custom User Agent
Edit `fetch-agent.js`:
```javascript
this.userAgent = 'YourCustomBot/1.0';
```

### Cache Settings
Proxy server caches up to 100 pages in memory.
Clear cache via API:
```bash
curl -X DELETE http://localhost:3000/api/cache
```

## üìÅ File Structure

```
ClaudeCodeBrowser/
‚îú‚îÄ‚îÄ index.html              # Web interface (updated with JSON support)
‚îú‚îÄ‚îÄ fetch-agent.js          # URL fetcher and HTML‚ÜíJSON converter
‚îú‚îÄ‚îÄ proxy-server.js         # Proxy server (bypasses CORS)
‚îú‚îÄ‚îÄ package.json            # Dependencies
‚îú‚îÄ‚îÄ pages/                  # Saved page JSON files (created automatically)
‚îÇ   ‚îú‚îÄ‚îÄ example_com_123.json
‚îÇ   ‚îú‚îÄ‚îÄ github_com_456.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ COMPLETE_SETUP.md       # This file
```

## üéØ Complete Workflows

### Workflow 1: Research Multiple Sites
```bash
# 1. Create URL list
cat > research.txt << EOF
https://en.wikipedia.org/wiki/Artificial_intelligence
https://en.wikipedia.org/wiki/Machine_learning
https://en.wikipedia.org/wiki/Deep_learning
EOF

# 2. Fetch all (saves to ./pages/)
node fetch-agent.js --batch research.txt

# 3. Start server
node proxy-server.js

# 4. Open http://localhost:3000
# 5. Go to "Sessions" ‚Üí "Load from Files"
# 6. Browse all fetched pages offline!
```

### Workflow 2: Automated Data Extraction
```bash
# 1. Fetch page
node fetch-agent.js https://news.site.com/article news.json

# 2. Extract data programmatically
node -e "
const data = require('./news.json');
console.log('Title:', data.content.title);
console.log('Paragraphs:', data.content.paragraphs.length);
console.log('Links:', data.content.links.length);
"

# 3. Process with your tools
cat news.json | jq '.content.paragraphs[]'
```

### Workflow 3: Build Dataset
```bash
# Fetch 100 pages
for i in {1..100}; do
  node fetch-agent.js "https://example.com/page/$i"
  sleep 1
done

# All saved to ./pages/
# Now you have a dataset of 100 pages as JSON!
```

## üî• Key Features

### ‚úÖ Complete CORS Bypass
- Fetch agent runs server-side (Node.js)
- No browser CORS restrictions
- Works with ANY website

### ‚úÖ Full Page Data
- HTML structure
- All text content
- Links, images, forms
- Meta tags
- Headers

### ‚úÖ Persistent Storage
- Save pages as JSON files
- Load offline
- Build datasets
- Version control friendly (JSON)

### ‚úÖ Fast & Cached
- Proxy server caches pages
- No re-fetching
- Instant load from cache

### ‚úÖ Programmable
- JSON format
- Easy to parse
- Script-friendly
- API endpoints

## üêõ Troubleshooting

### Port Already in Use
```bash
# Find and kill process
lsof -ti:3000 | xargs kill -9

# Or use different port
PORT=8080 node proxy-server.js
```

### Can't Fetch HTTPS Sites
Some sites require valid SSL. The fetch agent handles this automatically.

### Rate Limiting
Add delays in `fetch-agent.js`:
```javascript
await new Promise(r => setTimeout(r, 2000)); // 2 second delay
```

### Large Pages
Increase Node.js memory:
```bash
node --max-old-space-size=4096 fetch-agent.js <url>
```

## üìä Performance

- **Fetch Speed**: ~1-3 seconds per page
- **JSON Size**: ~10-50KB per page (vs 100-500KB HTML)
- **Cache**: 100 pages in memory
- **Batch**: Can fetch 1000s of pages

## üîê Security & Privacy

- Fetches happen server-side
- No cookies sent by default
- Respects robots.txt (you can modify)
- User agent customizable
- All data stays local

## üéì Next Steps

1. ‚úÖ **Start the server**: `node proxy-server.js`
2. ‚úÖ **Open interface**: http://localhost:3000
3. ‚úÖ **Fetch a page**: Enter URL and click Go
4. ‚úÖ **Extract data**: Use extraction tools
5. ‚úÖ **Save sessions**: Build your research library
6. ‚úÖ **Batch fetch**: Create datasets

## üí° Pro Tips

1. **Save Everything**: Use batch mode to fetch entire sites
2. **Version Control**: JSON files work great with Git
3. **Process Offline**: Fetch once, analyze many times
4. **Build Datasets**: Perfect for ML training data
5. **API Integration**: Use endpoints in your scripts

## üöÄ You're Ready!

The system is complete and fully functional. Start the server and begin browsing without CORS restrictions!

```bash
# Start browsing now:
node proxy-server.js

# Then open: http://localhost:3000
```

---

**Questions? Issues? Check the console output for debugging info!**
